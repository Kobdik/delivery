package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"os/signal"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
)

type DataCell struct {
	Id   string   `json:"id,omitempty"`
	Cmd  string   `json:"cmd,omitempty"`
	Day  int32    `json:"day,omitempty"`
	Mdt  string   `json:"mdt,omitempty"`
	Keys []string `json:"keys"`
	Val  int      `json:"val"`
}

type Topics []string

// String is an implementation of the flag.Value interface
func (imp *Topics) String() string {
	return fmt.Sprintf("%v", *imp)
}

// Set is an implementation of the flag.Value interface
func (imp *Topics) Set(value string) error {
	*imp = append(*imp, value)
	return nil
}

type Stock struct {
	// sync.RWMutex
	M    map[string]int
	cnt  int32
	flow chan DataCell
}

type Storage struct {
	sync.WaitGroup
	Topics    []string
	OutlayTop string
	SaldoPath string
	termChan  chan bool
	producer  *kafka.Producer
	stepDur   time.Duration
	saldo     []DataCell
	stocks    []Stock
	flow      chan *DataCell
	calday    atomic.Int32
	delays    int
	cost      int
	calc      bool
}

func main() {
	var topics Topics
	flag.Var(&topics, "t", "-t calend -t stocks")
	p_timer := flag.Int64("timer", 1000, "timer duration (ms)")
	p_path := flag.String("path", "./saldo.json", "path to saldo json file")
	p_ling := flag.Int("ling", 15, "linger (ms)")
	p_clean := flag.Bool("clean", false, "clean topics")
	p_calc := flag.Bool("calc", false, "calculate integral cost")
	flag.Parse()

	if len(topics) == 0 {
		fmt.Println("Define topics!")
		return
	}

	stg := &Storage{
		Topics:    topics,
		OutlayTop: "outlet",
		SaldoPath: *p_path,
		termChan:  make(chan bool),
		stepDur:   time.Duration(*p_timer * 1000000),
		stocks:    make([]Stock, 2),
		calc:      *p_calc,
	}

	go stg.waitTerm()

	if *p_clean {
		stg.cleanTopics()
		return
	}

	stg.loadSaldo()
	// prepare stocks
	stg.initStocks()

	conf_prod := &kafka.ConfigMap{
		"bootstrap.servers":  "localhost:9091,localhost:9092,localhost:9093",
		"client.id":          "storage",
		"enable.idempotence": true,
		"acks":               "all",
		"batch.size":         8192,
		"linger.ms":          *p_ling,
	}

	producer, err := kafka.NewProducer(conf_prod)
	if err != nil {
		fmt.Printf("Failed to create producer: %s\n", err)
		os.Exit(1)
	}
	defer producer.Close()
	stg.producer = producer

	for i := range 4 {
		stg.Add(1)
		go stg.readMessages(i)
	}

	stg.waitReaders()

	fmt.Println("Done!", stg.delays, len(stg.flow), stg.cost)
}

func (s *Storage) waitTerm() {
	signChan := make(chan os.Signal, 1)
	signal.Notify(signChan, syscall.SIGINT, syscall.SIGTERM)
	expired := time.After(10*time.Second + 60*s.stepDur)
	select {
	case <-signChan:
		fmt.Println("Terminated!")
	case <-expired:
		fmt.Println("Time elapsed!")
	}
	close(s.termChan)
}

func (s *Storage) waitReaders() {
	s.Wait()
	unflushed := s.producer.Flush(2000)
	for i := range s.stocks {
		fmt.Printf("Stock %d:\n", i)
		for mat, val := range s.stocks[i].M {
			fmt.Printf("Reserve of %s is\t%d\n", mat, val)
		}
	}
	fmt.Println("Stock readers finished", unflushed)
}

func (s *Storage) loadSaldo() {
	// executed only once
	data, err := os.ReadFile(s.SaldoPath)
	if err != nil {
		fmt.Printf("Can't read %s : %s", s.SaldoPath, err)
		return
	}

	s.saldo = make([]DataCell, 0, 6)
	err = json.Unmarshal(data, &s.saldo)
	if err != nil {
		fmt.Printf("Can't parse %s : %s", s.SaldoPath, err)
		return
	}
	fmt.Println("saldo length:", len(s.saldo))
}

func (c *DataCell) stIndex() (int, error) {
	// Keys[4] is st1 or st2
	switch c.Keys[4] {
	case "st1":
		return 0, nil
	case "st2":
		return 1, nil
	}
	return -1, fmt.Errorf("Invalid storage code: %s for cell %v\n", c.Keys[4], c)
}

func (s *Storage) initStocks() {
	for i := range 2 {
		s.stocks[i] = Stock{
			M: map[string]int{
				"cem":   0,
				"sand":  0,
				"stone": 0,
			},
			flow: make(chan DataCell, 2000),
		}
	}
	s.cost = 0
	s.delays = 0
	s.flow = make(chan *DataCell, 4000)

	for _, cell := range s.saldo {
		ind, err := cell.stIndex()
		if err != nil {
			fmt.Println(err.Error())
			continue
		}
		s.stocks[ind].M[cell.Keys[3]] += cell.Val
		s.flow <- &cell
	}

	for i := range s.stocks {
		fmt.Printf("Stock %d:\n", i)
		for mat, val := range s.stocks[i].M {
			fmt.Printf("Reserved %d of %s\n", val, mat)
		}
	}
}

func (s *Storage) integralCost(inst int) {
	for _, val := range s.stocks[inst].M {
		s.cost += val
	}
}

func (s *Storage) incrementStocks(cell *DataCell, day int32) {
	ind, err := cell.stIndex()
	if err != nil {
		fmt.Println(err.Error())
		return
	}
	if s.stocks[ind].M == nil {
		fmt.Printf("Supply was ignored: %v\n", cell)
		return
	}
	// just warning
	if cell.Day != day {
		fmt.Printf("%d income: %v\n", day, *cell)
	}
	// fmt.Printf("%s : %s inc %s storage %s\n", cell.Mdt, cell.Keys[1], cell.Keys[3], cell.Keys[4])
	// s.stocks[ind].Lock()
	s.stocks[ind].M[cell.Keys[3]] += cell.Val
	// s.stocks[ind].Unlock()
	s.flow <- cell
}

func (s *Storage) processDemand(cell *DataCell) {
	ind, err := cell.stIndex()
	if err != nil {
		fmt.Println(err.Error())
		return
	}
	// check daily sent counter
	cnt := s.stocks[ind].cnt
	if cnt > 120 {
		fmt.Println(cell.Mdt, "Too many demands!")
		return
	}
	// sure stok's reserve >= 0
	reserve := s.stocks[ind].M[cell.Keys[3]]
	if reserve < cell.Val {
		// fmt.Printf("%s Not enough %s in %s. Remains %d, but demands %d\n", mdt, cell.Keys[3], cell.Keys[4], reserve, cell.Val)
		s.delays++
		return
	}
	cell.Cmd = "outlay"
	top := &kafka.TopicPartition{Topic: &s.OutlayTop, Partition: kafka.PartitionAny}
	val, err := json.Marshal(*cell)
	if err != nil {
		fmt.Printf("Can't marshall outlay cell %v : %s\n", cell, err)
		return
	}
	s.producer.Produce(&kafka.Message{
		TopicPartition: *top,
		Value:          val,
	}, nil)
	// decrement stock's value
	s.stocks[ind].M[cell.Keys[3]] = reserve - cell.Val
	// increment sent counter
	s.stocks[ind].cnt++
	s.flow <- cell
}

func (s *Storage) doStock(inst int) {
	// separate group.id
	conf_cons := &kafka.ConfigMap{
		"bootstrap.servers": "localhost:9091,localhost:9092,localhost:9093",
		"client.id":         fmt.Sprintf("stock-cid-%d", inst),
		"group.id":          fmt.Sprintf("stock-gid-%d", inst),
		"auto.offset.reset": "latest",
	}
	// consumer within common group
	consumer, err := kafka.NewConsumer(conf_cons)
	if err != nil {
		fmt.Printf("Failed to create storage-0 consumer: %s\n", err)
		return
	}
	defer consumer.Close()
	// subscribe to calend and stocks topics
	err = consumer.Subscribe("calend", nil)
	if err != nil {
		fmt.Printf("Stock-%d failed to subscribe to calend topic, cause: %s\n", inst, err)
		return
	}
	fmt.Printf("Stock-%d subscribed to calend topic\n", inst)
	var (
		cell DataCell = DataCell{}
		cont bool     = true
		day  int32    = 0
	)
	for cont {
		select {
		case <-s.termChan:
			cont = false
			fmt.Printf("Stock-%d terminated!", inst)
		default:
			msg, err := consumer.ReadMessage(2 * s.stepDur)
			if err != nil {
				fmt.Printf("Stock-%d failed, cause %v\n", inst, err)
				continue
			}
			// messages separated with key: Partition-Keys[2] pairs
			// fmt.Printf("%s Topic %s[%d]-%d read %s\n", msg.Key, *msg.TopicPartition.Topic, msg.TopicPartition.Partition, msg.TopicPartition.Offset, string(msg.Value))
			err = json.Unmarshal(msg.Value, &cell)
			if err != nil {
				fmt.Printf("Can't parse : %s", err)
				continue
			}
			switch cell.Cmd {
			case "calend":
				// marshalled date
				s.calday.Store(cell.Day)
				// context variables
				day = cell.Day

				s.stocks[inst].cnt = 0
				fmt.Printf("Instance-%d day %d\n", inst, day)
				if s.calc {
					s.integralCost(inst)
				}
			default:
				fmt.Printf("Unknown command %v in cell\n", cell)
				continue
			}
		}
	}
	s.Done()
	fmt.Println("Calend reader done.")
}

func (s *Storage) readMessages(inst int) {
	// common group.id
	conf_cons := &kafka.ConfigMap{
		"bootstrap.servers": "localhost:9091,localhost:9092,localhost:9093",
		"client.id":         fmt.Sprintf("storage-%d", inst),
		"group.id":          "storage-grp",
		"auto.offset.reset": "latest",
	}
	// consumer within common group
	consumer, err := kafka.NewConsumer(conf_cons)
	if err != nil {
		fmt.Printf("Failed to create storage-%d consumer: %s\n", inst, err)
		return
	}
	defer consumer.Close()
	// subscribe to calend and stocks topics
	err = consumer.SubscribeTopics(s.Topics, nil)
	if err != nil {
		fmt.Printf("Storage-%d failed to subscribe to topics %v, cause: %s\n", inst, s.Topics, err)
		return
	}
	fmt.Printf("Reader-%d subscribed to topics %v\n", inst, s.Topics)
	var (
		cell *DataCell
		cont bool  = true
		day  int32 = 0
	)
	for cont {
		select {
		case <-s.termChan:
			cont = false
			fmt.Printf("Reader instance %d terminated!\n", inst)
		default:
			msg, err := consumer.ReadMessage(5 * s.stepDur)
			if err != nil {
				// fmt.Printf("Reader-%d failed, cause %v\n", inst, err)
				continue
			}
			// messages separated with key: Partition-Keys[2] pairs
			// fmt.Printf("%s Topic %s[%d]-%d read %s\n", msg.Key, *msg.TopicPartition.Topic, msg.TopicPartition.Partition, msg.TopicPartition.Offset, string(msg.Value))
			cell = new(DataCell)
			err = json.Unmarshal(msg.Value, cell)
			if err != nil {
				fmt.Printf("Can't parse : %s", err)
				continue
			}
			switch cell.Cmd {
			case "calend":
				// marshalled date
				s.calday.Store(cell.Day)
				// context variables
				day = cell.Day
				s.stocks[0].cnt = 0
				s.stocks[1].cnt = 0
				fmt.Printf("Instance-%d day %d\n", inst, day)
				if s.calc {
					// context agnostic
					s.integralCost(0)
				}
			// stocks topic
			case "supply":
				// fmt.Printf("Instance-%d Stock %d\n", inst, ind)
				// take all supplied materials as is
				// day = s.calday.Load()
				if cell.Day != day {
					day = s.calday.Load()
					fmt.Printf("Supply to Instance-%d day %d\n", inst, day)
				}
				s.incrementStocks(cell, day)
			case "demand":
				// ignore too late demands
				// day = s.calday.Load()
				if cell.Day != day {
					day = s.calday.Load()
					// fmt.Printf("Demand to Instance-%d day %d\n", inst, day)
				}
				if cell.Day != day {
					continue
				}
				// demands from outlet
				s.processDemand(cell)
			default:
				fmt.Printf("Unknown command %v in cell\n", cell)
				continue
			}
		}
	}
	s.Done()
	fmt.Printf("Reader-%d done.\n", inst)
}

func printDelivered(p *kafka.Producer) {
	cnt := 0
	for ev := range p.Events() {
		switch m := ev.(type) {
		case *kafka.Message:
			if m.TopicPartition.Error != nil {
				fmt.Printf("Delivery failed: %v\n", m.TopicPartition.Error)
			} else {
				fmt.Printf("%d Message with key %s delivered to partition [%d]\n%v\n",
					cnt, string(m.Key), m.TopicPartition.Partition, string(m.Value))
				cnt++
			}
		case kafka.Error:
			fmt.Printf("Kafka error: %v\n", ev)
		default:
			fmt.Printf("Ignored event: %s\n", ev)
		}
	}
	fmt.Println("Delivery channel closed.")
}

func (s *Storage) cleanTopics() {
	conf := &kafka.ConfigMap{
		"bootstrap.servers": "localhost:9091,localhost:9092,localhost:9093",
		"group.id":          "storage-grp",
		"auto.offset.reset": "latest",
	}
	consumer, err := kafka.NewConsumer(conf)
	if err != nil {
		fmt.Printf("Failed to create storage consumer: %s\n", err)
		return
	}
	defer consumer.Close()
	// subscribe to calend, supply, demand and outlay
	err = consumer.SubscribeTopics(s.Topics, nil)
	if err != nil {
		fmt.Printf("Storage failed to subscribe to topics %v, cause: %s\n", s.Topics, err)
		return
	}
	var parts []int = make([]int, 4)
	cont := true
	for cont {
		select {
		case <-s.termChan:
			cont = false
			fmt.Println("Reader terminated!")
		default:
			msg, err := consumer.ReadMessage(5 * time.Duration(s.stepDur))
			if err != nil {
				fmt.Printf("Reader failed, cause %v\n", err)
				continue
			}
			parts[msg.TopicPartition.Partition] += 1
			// messages separated with key: Partition-Keys[2] pairs
			// fmt.Printf("%s Topic %s[%d]-%d\n", msg.Key, *msg.TopicPartition.Topic, msg.TopicPartition.Partition, msg.TopicPartition.Offset)
			fmt.Printf("%s Topic %s[%d]-%d read %s\n", msg.Key, *msg.TopicPartition.Topic, msg.TopicPartition.Partition, msg.TopicPartition.Offset, string(msg.Value))
		}
	}
	for i, p := range parts {
		fmt.Printf("[%d]: %d\n", i, p)
	}

	fmt.Println("Cleaned")
}
